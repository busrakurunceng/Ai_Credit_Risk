{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home Credit Default Risk — Preprocessing\n",
    "\n",
    "**Project:** AI-Powered Intelligent Risk Management System  \n",
    "**Dataset:** [Kaggle - Home Credit Default Risk](https://www.kaggle.com/competitions/home-credit-default-risk)  \n",
    "**Input:** `data/train_featured.parquet` (307,511 × 212) | `data/test_featured.parquet` (48,744 × 211)  \n",
    "**Objective:** Validate full-data statistics, handle missing values, encode categoricals, remove multicollinearity, and prepare the final model-ready dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. [Libraries & Configuration](#1)\n",
    "2. [Data Loading](#2)\n",
    "3. [Full-Data Validation (EDA Sanity Check)](#3)\n",
    "4. [Missing Value Strategy](#4)\n",
    "5. [Outlier Handling](#5)\n",
    "6. [Categorical Encoding](#6)\n",
    "7. [Multicollinearity Check](#7)\n",
    "8. [Final Dataset & Export](#8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "## 1. Libraries & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory : c:\\Users\\busra\\Projects\\Ai_Credit_Risk\\data\n",
      "Plot directory : c:\\Users\\busra\\Projects\\Ai_Credit_Risk\\notebooks\\plots\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Add project root to path for local imports\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = os.path.join(PROJECT_ROOT, 'data')\n",
    "PLOT_DIR = os.path.join(PROJECT_ROOT, 'notebooks', 'plots')\n",
    "os.makedirs(PLOT_DIR, exist_ok=True)\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "\n",
    "# Plot style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(f\"Data directory : {DATA_DIR}\")\n",
    "print(f\"Plot directory : {PLOT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "## 2. Data Loading\n",
    "\n",
    "Feature Engineering pipeline çıktısı olan parquet dosyalarını yüklüyoruz.  \n",
    "Train: 307,511 × 212 | Test: 48,744 × 211 (TARGET sütunu yok)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ArrowKeyError",
     "evalue": "A type extension with name pandas.period already defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mArrowKeyError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m train = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATA_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrain_featured.parquet\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m test  = pd.read_parquet(os.path.join(DATA_DIR, \u001b[33m'\u001b[39m\u001b[33mtest_featured.parquet\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTrain shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\busra\\Projects\\Ai_Credit_Risk\\venv\\Lib\\site-packages\\pandas\\io\\parquet.py:668\u001b[39m, in \u001b[36mread_parquet\u001b[39m\u001b[34m(path, engine, columns, storage_options, dtype_backend, filesystem, filters, to_pandas_kwargs, **kwargs)\u001b[39m\n\u001b[32m    508\u001b[39m \u001b[38;5;129m@set_module\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mpandas\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    509\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mread_parquet\u001b[39m(\n\u001b[32m    510\u001b[39m     path: FilePath | ReadBuffer[\u001b[38;5;28mbytes\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    518\u001b[39m     **kwargs,\n\u001b[32m    519\u001b[39m ) -> DataFrame:\n\u001b[32m    520\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    521\u001b[39m \u001b[33;03m    Load a parquet object from the file path, returning a DataFrame.\u001b[39;00m\n\u001b[32m    522\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    665\u001b[39m \u001b[33;03m    1    4    9\u001b[39;00m\n\u001b[32m    666\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m668\u001b[39m     impl = \u001b[43mget_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    669\u001b[39m     check_dtype_backend(dtype_backend)\n\u001b[32m    671\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m impl.read(\n\u001b[32m    672\u001b[39m         path,\n\u001b[32m    673\u001b[39m         columns=columns,\n\u001b[32m   (...)\u001b[39m\u001b[32m    679\u001b[39m         **kwargs,\n\u001b[32m    680\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\busra\\Projects\\Ai_Credit_Risk\\venv\\Lib\\site-packages\\pandas\\io\\parquet.py:64\u001b[39m, in \u001b[36mget_engine\u001b[39m\u001b[34m(engine)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m engine_class \u001b[38;5;129;01min\u001b[39;00m engine_classes:\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mengine_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     65\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m     66\u001b[39m         error_msgs += \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m - \u001b[39m\u001b[33m\"\u001b[39m + \u001b[38;5;28mstr\u001b[39m(err)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\busra\\Projects\\Ai_Credit_Risk\\venv\\Lib\\site-packages\\pandas\\io\\parquet.py:170\u001b[39m, in \u001b[36mPyArrowImpl.__init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    167\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyarrow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparquet\u001b[39;00m\n\u001b[32m    169\u001b[39m \u001b[38;5;66;03m# import utils to register the pyarrow extension types\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marrays\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marrow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mextension_types\u001b[39;00m  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[39;00m\n\u001b[32m    172\u001b[39m \u001b[38;5;28mself\u001b[39m.api = pyarrow\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\busra\\Projects\\Ai_Credit_Risk\\venv\\Lib\\site-packages\\pandas\\core\\arrays\\arrow\\extension_types.py:59\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# register the type with a dummy instance\u001b[39;00m\n\u001b[32m     58\u001b[39m _period_type = ArrowPeriodType(\u001b[33m\"\u001b[39m\u001b[33mD\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m \u001b[43mpyarrow\u001b[49m\u001b[43m.\u001b[49m\u001b[43mregister_extension_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_period_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mArrowIntervalType\u001b[39;00m(pyarrow.ExtensionType):\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, subtype, closed: IntervalClosedType) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     64\u001b[39m         \u001b[38;5;66;03m# attributes need to be set first before calling\u001b[39;00m\n\u001b[32m     65\u001b[39m         \u001b[38;5;66;03m# super init (as that calls serialize)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\busra\\Projects\\Ai_Credit_Risk\\venv\\Lib\\site-packages\\pyarrow\\types.pxi:2226\u001b[39m, in \u001b[36mpyarrow.lib.register_extension_type\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\busra\\Projects\\Ai_Credit_Risk\\venv\\Lib\\site-packages\\pyarrow\\error.pxi:92\u001b[39m, in \u001b[36mpyarrow.lib.check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mArrowKeyError\u001b[39m: A type extension with name pandas.period already defined"
     ]
    }
   ],
   "source": [
    "train = pd.read_parquet(os.path.join(DATA_DIR, 'train_featured.parquet'))\n",
    "test  = pd.read_parquet(os.path.join(DATA_DIR, 'test_featured.parquet'))\n",
    "\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Test  shape: {test.shape}\")\n",
    "print(f\"\\nTrain memory: {train.memory_usage(deep=True).sum() / 1e6:.1f} MB\")\n",
    "print(f\"Test  memory: {test.memory_usage(deep=True).sum() / 1e6:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "## 3. Full-Data Validation (EDA Sanity Check)\n",
    "\n",
    "EDA %20 örneklem üzerinde yapılmıştı. Şimdi tam veri üzerinde temel bulguları doğruluyoruz:  \n",
    "- Sınıf dağılımı (TARGET oranı)  \n",
    "- Eksik değer haritası  \n",
    "- Sayısal dağılım istatistikleri (aykırı değer tespiti)  \n",
    "- Kategorik sütun dağılımları"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 — Target Distribution (Sınıf Dengesi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sınıf dağılımı — EDA'da %20 örneklemde ~%8 default bulunmuştu\n",
    "target_counts = train['TARGET'].value_counts()\n",
    "target_pct    = train['TARGET'].value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"TARGET DISTRIBUTION (Full Data)\")\n",
    "print(\"=\" * 50)\n",
    "for val in [0, 1]:\n",
    "    print(f\"  TARGET={val}: {target_counts[val]:>7,} ({target_pct[val]:.2f}%)\")\n",
    "print(f\"  Ratio: 1:{target_counts[0] / target_counts[1]:.1f}\")\n",
    "print(f\"  Total: {len(train):,}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Visualization\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "colors = ['#2ecc71', '#e74c3c']\n",
    "target_counts.plot(kind='bar', color=colors, edgecolor='black', ax=ax)\n",
    "ax.set_title('Target Distribution (Full Data)', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('TARGET')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_xticklabels(['0 (No Default)', '1 (Default)'], rotation=0)\n",
    "for i, (count, pct) in enumerate(zip(target_counts, target_pct)):\n",
    "    ax.text(i, count + 2000, f'{count:,}\\n({pct:.1f}%)', ha='center', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 — Missing Value Map (Eksik Değer Haritası)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eksik değer analizi — tam veri üzerinde\n",
    "missing = train.isnull().sum()\n",
    "missing_pct = (missing / len(train)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'missing_count': missing,\n",
    "    'missing_pct': missing_pct\n",
    "}).query('missing_count > 0').sort_values('missing_pct', ascending=False)\n",
    "\n",
    "print(f\"Total columns with missing values: {len(missing_df)} / {train.shape[1]}\")\n",
    "print(f\"Columns with >50% missing: {(missing_df['missing_pct'] > 50).sum()}\")\n",
    "print(f\"Columns with 20-50% missing: {((missing_df['missing_pct'] > 20) & (missing_df['missing_pct'] <= 50)).sum()}\")\n",
    "print(f\"Columns with <20% missing: {(missing_df['missing_pct'] <= 20).sum()}\")\n",
    "print()\n",
    "\n",
    "# Kategoriye göre grupla\n",
    "print(\"=\" * 70)\n",
    "print(\"COLUMNS WITH >50% MISSING (drop candidates)\")\n",
    "print(\"=\" * 70)\n",
    "high_missing = missing_df[missing_df['missing_pct'] > 50]\n",
    "if len(high_missing) > 0:\n",
    "    for col, row in high_missing.iterrows():\n",
    "        print(f\"  {col:<45} {row['missing_pct']:>6.1f}%  ({row['missing_count']:>7,.0f})\")\n",
    "else:\n",
    "    print(\"  None\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "print(\"COLUMNS WITH 20-50% MISSING\")\n",
    "print(\"=\" * 70)\n",
    "mid_missing = missing_df[(missing_df['missing_pct'] > 20) & (missing_df['missing_pct'] <= 50)]\n",
    "if len(mid_missing) > 0:\n",
    "    for col, row in mid_missing.iterrows():\n",
    "        print(f\"  {col:<45} {row['missing_pct']:>6.1f}%  ({row['missing_count']:>7,.0f})\")\n",
    "else:\n",
    "    print(\"  None\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "print(\"COLUMNS WITH <20% MISSING\")\n",
    "print(\"=\" * 70)\n",
    "low_missing = missing_df[missing_df['missing_pct'] <= 20]\n",
    "if len(low_missing) > 0:\n",
    "    for col, row in low_missing.iterrows():\n",
    "        print(f\"  {col:<45} {row['missing_pct']:>6.1f}%  ({row['missing_count']:>7,.0f})\")\n",
    "else:\n",
    "    print(\"  None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing value heatmap — en çok eksik olan ilk 30 sütun\n",
    "top_missing = missing_df.head(30)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "colors = ['#e74c3c' if pct > 50 else '#f39c12' if pct > 20 else '#3498db'\n",
    "          for pct in top_missing['missing_pct']]\n",
    "bars = ax.barh(range(len(top_missing)), top_missing['missing_pct'], color=colors, edgecolor='black', linewidth=0.5)\n",
    "ax.set_yticks(range(len(top_missing)))\n",
    "ax.set_yticklabels(top_missing.index, fontsize=9)\n",
    "ax.set_xlabel('Missing %')\n",
    "ax.set_title('Top 30 Columns by Missing Percentage (Full Data)', fontsize=14, fontweight='bold')\n",
    "ax.axvline(x=50, color='red', linestyle='--', alpha=0.7, label='50% threshold')\n",
    "ax.axvline(x=20, color='orange', linestyle='--', alpha=0.7, label='20% threshold')\n",
    "ax.legend()\n",
    "ax.invert_yaxis()\n",
    "\n",
    "for i, (pct, count) in enumerate(zip(top_missing['missing_pct'], top_missing['missing_count'])):\n",
    "    ax.text(pct + 0.5, i, f'{pct:.1f}%', va='center', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 — Numerical Distribution Statistics (Aykırı Değer Tespiti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sayısal sütunların istatistikleri — aykırı değer tespiti için\n",
    "num_cols = train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "num_cols = [c for c in num_cols if c not in ['SK_ID_CURR', 'TARGET']]\n",
    "\n",
    "print(f\"Numerical columns (excluding SK_ID_CURR, TARGET): {len(num_cols)}\")\n",
    "print()\n",
    "\n",
    "# Persentil tablosu\n",
    "desc = train[num_cols].describe(percentiles=[.01, .05, .25, .5, .75, .95, .99]).T\n",
    "desc['iqr'] = desc['75%'] - desc['25%']\n",
    "desc['range'] = desc['max'] - desc['min']\n",
    "desc['skew'] = train[num_cols].skew()\n",
    "\n",
    "# Aşırı çarpık veya aşırı geniş range'e sahip sütunlar\n",
    "print(\"=\" * 70)\n",
    "print(\"HIGHLY SKEWED COLUMNS (|skew| > 5) — log transform candidates\")\n",
    "print(\"=\" * 70)\n",
    "skewed = desc[desc['skew'].abs() > 5].sort_values('skew', ascending=False)\n",
    "if len(skewed) > 0:\n",
    "    print(skewed[['mean', 'std', 'min', '50%', '99%', 'max', 'skew']].to_string())\n",
    "else:\n",
    "    print(\"  None\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "print(\"EXTREME OUTLIER COLUMNS (max > 10x 99th percentile)\")\n",
    "print(\"=\" * 70)\n",
    "extreme = desc[desc['max'] > desc['99%'] * 10]\n",
    "if len(extreme) > 0:\n",
    "    print(extreme[['mean', '99%', 'max', 'skew']].to_string())\n",
    "else:\n",
    "    print(\"  None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 — Categorical Column Overview (Kategorik Dağılımlar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kategorik sütunları tespit et\n",
    "cat_cols = train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(f\"Categorical columns: {len(cat_cols)}\")\n",
    "print()\n",
    "\n",
    "if len(cat_cols) > 0:\n",
    "    cat_summary = pd.DataFrame({\n",
    "        'dtype': train[cat_cols].dtypes,\n",
    "        'nunique': train[cat_cols].nunique(),\n",
    "        'missing_pct': (train[cat_cols].isnull().sum() / len(train) * 100).round(2),\n",
    "        'top_value': [train[c].mode().iloc[0] if not train[c].mode().empty else 'N/A' for c in cat_cols],\n",
    "        'top_freq_pct': [(train[c].value_counts(normalize=True).iloc[0] * 100).round(1)\n",
    "                         if train[c].notna().any() else 0 for c in cat_cols]\n",
    "    }).sort_values('nunique', ascending=False)\n",
    "\n",
    "    print(cat_summary.to_string())\n",
    "    print()\n",
    "\n",
    "    # Çok az kategorili sütunlar (binary veya 3-4 kategorili) — Label Encoding adayları\n",
    "    print(\"=\" * 70)\n",
    "    print(\"BINARY / LOW-CARDINALITY COLUMNS (nunique <= 4) — Label Encoding candidates\")\n",
    "    print(\"=\" * 70)\n",
    "    low_card = cat_summary[cat_summary['nunique'] <= 4]\n",
    "    for col in low_card.index:\n",
    "        vals = train[col].value_counts(dropna=False).head(5)\n",
    "        print(f\"\\n  {col} (nunique={low_card.loc[col, 'nunique']}):\")\n",
    "        for v, cnt in vals.items():\n",
    "            print(f\"    {str(v):<30} {cnt:>7,} ({cnt/len(train)*100:.1f}%)\")\n",
    "\n",
    "    print()\n",
    "    print(\"=\" * 70)\n",
    "    print(\"HIGH-CARDINALITY COLUMNS (nunique > 4) — Target/Frequency Encoding candidates\")\n",
    "    print(\"=\" * 70)\n",
    "    high_card = cat_summary[cat_summary['nunique'] > 4]\n",
    "    for col in high_card.index:\n",
    "        print(f\"  {col}: {high_card.loc[col, 'nunique']} unique values\")\n",
    "else:\n",
    "    print(\"  No categorical columns found (all may have been encoded already).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 — EDA vs Full Data Comparison Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA bulgularıyla karşılaştırma özeti\n",
    "print(\"=\" * 70)\n",
    "print(\"EDA (%20 SAMPLE) vs FULL DATA — COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "default_rate = train['TARGET'].mean() * 100\n",
    "print(f\"\\n  Default rate:          EDA ~8.0%  |  Full data: {default_rate:.2f}%\")\n",
    "\n",
    "# EXT_SOURCE korelasyonları\n",
    "for col in ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']:\n",
    "    if col in train.columns:\n",
    "        corr = train[col].corr(train['TARGET'])\n",
    "        print(f\"  {col} corr:  EDA ~-0.16  |  Full data: {corr:.4f}\")\n",
    "\n",
    "# Toplam eksik sütun sayısı\n",
    "n_missing_cols = (train.isnull().sum() > 0).sum()\n",
    "n_high_missing = (train.isnull().sum() / len(train) > 0.50).sum()\n",
    "print(f\"\\n  Columns with any missing:  {n_missing_cols}\")\n",
    "print(f\"  Columns with >50% missing: {n_high_missing}\")\n",
    "print(f\"  Total columns:             {train.shape[1]}\")\n",
    "print(f\"  Total rows:                {train.shape[0]:,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"VALIDATION COMPLETE — Ready for preprocessing.\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='4'></a>\n",
    "## 4. Missing Value Strategy\n",
    "\n",
    "*TODO — Next step*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5'></a>\n",
    "## 5. Outlier Handling\n",
    "\n",
    "*TODO*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='6'></a>\n",
    "## 6. Categorical Encoding\n",
    "\n",
    "*TODO*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='7'></a>\n",
    "## 7. Multicollinearity Check\n",
    "\n",
    "*TODO*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='8'></a>\n",
    "## 8. Final Dataset & Export\n",
    "\n",
    "*TODO*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
